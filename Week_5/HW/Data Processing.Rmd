---
title: "HW5"
author: "Louie Chen"
date: "October 18, 2018"
output: html_document
---
##研究問題與動機
目前已經知道許多媒體在報導政治人物或政治事件時會透過自己主觀的想法將報導內容偏向自己支持的一方，但是在比較沒有政治色彩的其他人物報導上會不會有這種現象呢?
Tesla CEO: Elon Musk是一個企業人士中較為一體兩面的人，優點缺點鮮明，因此想要捧他或想要黑他都很可以發揮，因此我挑選這個人作為研究題材。希望透過這個人看看每個媒體的報導的著重點是否受其立場影響。

##Load Library
```{r}
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(plyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
library(wordcloud)
library(ggplot2)
```

##Data Preprocessing
```{r}
#Set Language as traditional Chinese
Sys.setlocale(category = "LC_ALL", locale = "cht")

#Load Data
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
```

## data cleaning
```{r}
toSpace <- content_transformer(function(x, pattern) {
  return (gsub(pattern, " ", x))
})
##困難：有一些已經去除的字在後面還是持續出現，不知為何
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "分享平台")
docs <- tm_map(docs, toSpace, "人人網")
docs <- tm_map(docs, toSpace, "開心網")
docs <- tm_map(docs, toSpace, "微博")
docs <- tm_map(docs, toSpace, "豆瓣")
docs <- tm_map(docs, toSpace, "複製鏈接")
docs <- tm_map(docs, toSpace, "這是外部鏈接，瀏覽器將打開另一個窗口")
docs <- tm_map(docs, toSpace, "大紀元")
docs <- tm_map(docs, toSpace, "年")
docs <- tm_map(docs, toSpace, "月")
docs <- tm_map(docs, toSpace, "日")
docs <- tm_map(docs, toSpace, "訊")
docs <- tm_map(docs, toSpace, "【")
docs <- tm_map(docs, toSpace, "】")
docs <- tm_map(docs, toSpace, "（")
docs <- tm_map(docs, toSpace, "）")
docs <- tm_map(docs, toSpace, "綜合")
docs <- tm_map(docs, toSpace, "報導")
docs <- tm_map(docs, toSpace, "影音")
docs <- tm_map(docs, toSpace, "評論")
docs <- tm_map(docs, toSpace, "健康")
docs <- tm_map(docs, toSpace, "責任編輯")
docs <- tm_map(docs, toSpace, "<")
docs <- tm_map(docs, toSpace, ">")
docs <- tm_map(docs, toSpace, "維基百科")
docs <- tm_map(docs, toSpace, "隨時關注最新創業、科技、網路、工作訊息。")
docs <- tm_map(docs, toSpace, "延伸閱讀與參考：")
docs <- tm_map(docs, toSpace, "分享文章或觀看評論")
docs <- tm_map(docs, toSpace, "硬塞的網路趨勢觀察")
docs <- tm_map(docs, toSpace, "記者")
docs <- tm_map(docs, toSpace, "我們為什麼挑選這篇文章")
docs <- tm_map(docs, toSpace, "本文經合作夥伴")
docs <- tm_map(docs, toSpace, "授權轉載")
docs <- tm_map(docs, toSpace, "並同意 TechOrange 編寫導讀與修訂標題")
docs <- tm_map(docs, toSpace, "原文標題為")
docs <- tm_map(docs, toSpace, "1. 高度關注國際科技趨勢、台灣產業新聞 
2. 根據月度編輯台企劃，執行編輯、採訪與撰稿工作 
3. 進行線上、線下媒體策展 
4. 根據不同策展專案進行跨部門溝通 
5. 針對網站數據做解讀與優化分析 
6. 具有 2~3 年工作經驗的媒體工作者 
7. 習慣閱讀《彭博社》、《財富雜誌》、《金融時報》、《Fast Company》者更佳 
8. 目標導向思考，對準目標、彈性工作")
docs <- tm_map(docs, toSpace, "意者請提供履歷自傳以及「相關文字作品」，寄至  [email<U+00A0>protected]。來信主旨請註明：【應徵】TechOrange 社群編輯：您的大名")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "在")
docs <- tm_map(docs, toSpace, "馬斯克")
docs <- tm_map(docs, toSpace, "了")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "他")
docs <- tm_map(docs, toSpace, "為")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "但")
docs <- tm_map(docs, toSpace, "對")
docs <- tm_map(docs, toSpace, "與")
docs <- tm_map(docs, toSpace, "就")
docs <- tm_map(docs, toSpace, "慎")
docs <- tm_map(docs, toSpace, "已經")
docs <- tm_map(docs, toSpace, "五")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "可能")
docs <- tm_map(docs, toSpace, "可以")
docs <- tm_map(docs, toSpace, "而")
docs <- tm_map(docs, toSpace, "讓")
docs <- tm_map(docs, toSpace, "這")
docs <- tm_map(docs, toSpace, "個")
docs <- tm_map(docs, toSpace, "你")
docs <- tm_map(docs, toSpace, "他們")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "以")
docs <- tm_map(docs, toSpace, "後")
docs <- tm_map(docs, toSpace, "被")
docs <- tm_map(docs, toSpace, "們")
docs <- tm_map(docs, toSpace, "有")
docs <- tm_map(docs, toSpace, "說")
docs <- tm_map(docs, toSpace, "都")
docs <- tm_map(docs, toSpace, "有")
docs <- tm_map(docs, toSpace, "進行")
docs <- tm_map(docs, toSpace, "㸱")
docs <- tm_map(docs, toSpace, "忖")
docs <- tm_map(docs, toSpace, "丹")
docs <- tm_map(docs, toSpace, "其")
docs <- tm_map(docs, toSpace, "及")
docs <- tm_map(docs, toSpace, "將")
docs <- tm_map(docs, toSpace, "些")
docs <- tm_map(docs, toSpace, "還")

##困難：這裡已經把英文去除，隨然在詞頻矩陣已看不到英文，但在最後Output的檔案中還是有英文
docs <- tm_map(docs, toSpace, "[a-zA-Z]")

# words cut
##困難：原本想把斷詞都存在CSV檔，但是存入後用Excel打開都變成亂碼，所以放棄這樣做
#keywords = read.csv("keywords.csv")
mixseg = worker()
#keys = as.matrix(keywords)
#new_user_word(mixseg, keys)

##困難：這裡加了自訂的斷詞，但是有些字還是沒有如預期的那樣斷整齊
new_user_word(mixseg, c("私有化", "公布", "宣布", "發布", "特斯拉", "伊隆", "馬斯克", "降低", "相互", "互聯網", "互動", "中國", "獵鷹重型"))

jieba_tokenizer = function(d){
  unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
```

##資料整理、視覺化
```{r}
#詞頻矩陣(整合)
##困難：有出現一些原本沒有在文本中的字，例如：慎、丰，不知為何?
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame, 30), format = "markdown")

#Word Cloud
wordcloud(freqFrame$Var1,freqFrame$Freq,
          scale=c(5,0.1),min.freq=49,max.words=500,
          random.order=TRUE, random.color=FALSE, 
          rot.per=.1, colors=brewer.pal(8, "Dark2"),
          ordered.colors=FALSE,use.r.layout=FALSE,
          fixed.asp=TRUE)

#Draw Bar Chart

#BBC Chinese
seg1 = lapply(docs[1], jieba_tokenizer)
freqFrame1 = as.data.frame(table(unlist(seg1)))
freqFrame1 = freqFrame1[order(freqFrame1$Freq,decreasing=TRUE), ]
headfreq1 = head(freqFrame1,20)
headfreq1$Var1 <- factor(headfreq1$Var1, levels = headfreq1$Var1)
ggplot(data = headfreq1, aes(x = Var1, y = Freq)) +
  geom_bar(stat="identity", width = 0.5, fill = "red", colour = "tomato2") + 
  labs(title="Hot Words Ranking for Elon Musk", 
       subtitle="Sorted Bar Chart", 
       caption="Source: BBC Chinese") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))

#Epochtimes
seg2 = lapply(docs[2], jieba_tokenizer)
freqFrame2 = as.data.frame(table(unlist(seg2)))
freqFrame2 = freqFrame2[order(freqFrame2$Freq,decreasing=TRUE), ]
headfreq2 = head(freqFrame2,20)
headfreq2$Var1 <- factor(headfreq2$Var1, levels = headfreq2$Var1)
ggplot(data = headfreq2, aes(x = Var1, y = Freq)) +
  geom_bar(stat="identity", width = 0.5, fill = "orange") + 
  labs(title="Hot Words Ranking for Elon Musk", 
       subtitle="Sorted Bar Chart", 
       caption="Source: Epochtimes") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))

#Inside
seg3 = lapply(docs[3], jieba_tokenizer)
freqFrame3 = as.data.frame(table(unlist(seg3)))
freqFrame3 = freqFrame3[order(freqFrame3$Freq,decreasing=TRUE), ]
headfreq3 = head(freqFrame3,20)
headfreq3$Var1 <- factor(headfreq3$Var1, levels = headfreq3$Var1)
ggplot(data = headfreq3, aes(x = Var1, y = Freq)) +
  geom_bar(stat="identity", width = 0.5, fill = "yellow") + 
  labs(title="Hot Words Ranking for Elon Musk", 
       subtitle="Sorted Bar Chart", 
       caption="Source: Inside") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))

#Techorange
seg4 = lapply(docs[4], jieba_tokenizer)
freqFrame4 = as.data.frame(table(unlist(seg4)))
freqFrame4 = freqFrame4[order(freqFrame4$Freq,decreasing=TRUE), ]
headfreq4 = head(freqFrame4,20)
headfreq4$Var1 <- factor(headfreq4$Var1, levels = headfreq4$Var1)
ggplot(data = headfreq4, aes(x = Var1, y = Freq)) +
  geom_bar(stat="identity", width = 0.5, fill = "green") + 
  labs(title="Hot Words Ranking for Elon Musk", 
       subtitle="Sorted Bar Chart", 
       caption="Source: Techorange") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))

#詞頻矩陣(個別)
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
tf <- as.matrix(tdm)
DF <- tidy(tf)
```

## tf-idf computation
```{r}
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{ 
  log2( N / nnzero(word_doc) ) 
}
idf <- apply(tdm, 1, idfCal)

doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
  for(y in 1:ncol(tdm))
  {
    doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
  }
}

findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn = doc.tfidf[-which(findZeroId == 0),]

```

##Outout File
```{r}
write.table(tfidfnn, 'show.txt')
write.csv(tfidfnn, "show.csv")
```

##結論
從結果看來，BBC提到較多人工智慧、霍金、人類等字眼，推測BBC在提到Musk時，著重的角度是在人類科技的進步與推展，因此會與創新科技、其他科學家、站在人類發展角度進行報導，進而再提到Musk這號人物。

Epochtimes在報導Musk則較為偏向介紹他個人為主，以這個人為核心進行報導，因此也會提到他所創辦的其他公司，例如：SpaceX，而不僅限於Tesla。

Inside則對Tesla這間公司較感興趣，因此是從Tesla這間公司的角度切入，再介紹到他們的CEO，並且對於財務與公司營運方面較為感興趣，相較之下比較不是提到技術的演進，因此字詞中較常提到：特斯拉私有化、股東、資金、美元、董事。

Techorange則是站在比較中性世界觀的觀點報導Tesla，再進而提到Musk這位領導者，可以發現他的報導方式並不只圍繞著美國電動車的發展，而會把中國的電動車、網路產業納入討論。

