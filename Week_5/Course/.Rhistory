#å®è®æ¸-é¡å¥å:bar chart
ggplot(data = airquality, aes(x = Temp)) +
geom_bar(fill = "lightblue", colour = "black")
#å®è®æ¸-é¡å¥å:bar chart
library(ggplot2)
ggplot(data = airquality, aes(x = Temp)) +
geom_bar(fill = "lightblue", colour = "black")
library(ggplot2)
ggplot(data = metro_bike_share_trip_data, aes(x = Duration)) +
geom_bar(fill = "lightblue", colour = "black")
metro_bike_share_trip_data[2]
nrow <- nrow(metro_bike_share_trip_data)
#Import data
##Method 1
'library(gdata)                   # load gdata package
help(read.xls)                  # documentation
mydata = read.csv("D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_3/Course/metro-bike-share-trip-data.csv")
mydata'
##Method 2
library(readr)
metro_bike_share_trip_data <- read_csv("D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_3/Course/metro-bike-share-trip-data.csv")
'View(metro_bike_share_trip_data)'
library(ggplot2)
#Import data
##Method 1
'library(gdata)                   # load gdata package
help(read.xls)                  # documentation
mydata = read.csv("D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_3/Course/metro-bike-share-trip-data.csv")
mydata'
##Method 2
library(readr)
metro_bike_share_trip_data <- read_csv("D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_3/Course/metro-bike-share-trip-data.csv")
'View(metro_bike_share_trip_data)'
library(ggplot2)
##Method 2
library(readr)
metro_bike_share_trip_data <- read_csv("D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_3/Course/metro-bike-share-trip-data.csv")
'View(metro_bike_share_trip_data)'
library(ggplot2)
nrow = nrow(metro_bike_share_trip_data)
for(i in c(1:nrow)){
cat(i)
}
nrow = nrow(metro_bike_share_trip_data)
for(i in c(1:10)){
cat(i)
}
metro_bike_share_trip_data[1,6]
'no_na_Starting_Station_Latitude <- na.omit(metro_bike_share_trip_data[6])
no_na_Starting_Station_Longitude <- na.omit(metro_bike_share_trip_data[7])
Latitude.list <- split(no_na_Starting_Station_Latitude, seq(nrow(no_na_Starting_Station_Latitude)))
Longitude.list <- split(no_na_Starting_Station_Longitude, seq(nrow(no_na_Starting_Station_Longitude)))
ggplot(data = metro_bike_share_trip_data, mapping = aes(x = Longitude.list), y = Latitude.list) +
geom_point(alpha = 0.1, color = "blue")'
list1 <- list(1,2,3,4,5)
list2 <- list(6,7,8,9)
list3 <- append(list1,list2)
list3
source('pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
#https://pecu.gitbooks.io/-r/content/zi-liao-ke-xue-yu-ji-qi-xue-xi.html
install.packages("tmcn")
source('pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
source('pttTestFunction.R')
#https://pecu.gitbooks.io/-r/content/zi-liao-ke-xue-yu-ji-qi-xue-xi.html
install.packages("tmcn")
source('pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
setwd("~/")
setwd("~/")
#https://pecu.gitbooks.io/-r/content/zi-liao-ke-xue-yu-ji-qi-xue-xi.html
#install.packages("tmcn")
source('pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
#https://pecu.gitbooks.io/-r/content/zi-liao-ke-xue-yu-ji-qi-xue-xi.html
#install.packages("tmcn")
library(tmcn)
source('pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
#https://pecu.gitbooks.io/-r/content/zi-liao-ke-xue-yu-ji-qi-xue-xi.html
#install.packages("tmcn")
source('pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
#https://pecu.gitbooks.io/-r/content/zi-liao-ke-xue-yu-ji-qi-xue-xi.html
#install.packages("tmcn")
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.1),min.freq=50,max.words=150,
random.order=TRUE, random.color=FALSE,
rot.per=.1, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
library(NLP)        # install.packages("NLP")
install.packages("NLP")
install.packages("tm")
install.packages("jiebaRD")
install.packages("jiebaR")
install.packages("jiebaR")
install.packages("wordcloud")
#https://pecu.gitbooks.io/-r/content/zi-liao-ke-xue-yu-ji-qi-xue-xi.html
#install.packages("tmcn")
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
#https://pecu.gitbooks.io/-r/content/zi-liao-ke-xue-yu-ji-qi-xue-xi.html
#install.packages("tmcn")
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
#https://pecu.gitbooks.io/-r/content/zi-liao-ke-xue-yu-ji-qi-xue-xi.html
#install.packages("tmcn")
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction,
URL = URL, filename = filename)
#install.packages("tmcn")
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:10)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:3)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction, URL = URL, filename = filename)
rm(list=ls(all.names = TRUE))
library(NLP)        # install.packages("NLP")
library(tm)         # install.packages("tm")
library(jiebaRD)    # install.packages("jiebaRD")
library(jiebaR)     # install.packages("jiebaR") 中文文字斷詞
library(RColorBrewer)
library(wordcloud)  #install.packages("wordcloud")
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, removePunctuation)
docs
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame, 10), format = "markdown")
par(family=("Heiti TC Light"))
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.1),min.freq=50,max.words=150,
random.order=TRUE, random.color=FALSE,
rot.per=.1, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:3)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction, URL = URL, filename = filename)
rm(list=ls(all.names = TRUE))
library(NLP)        # install.packages("NLP")
library(tm)         # install.packages("tm")
library(jiebaRD)    # install.packages("jiebaRD")
library(jiebaR)     # install.packages("jiebaR") 中文文字斷詞
library(RColorBrewer)
library(wordcloud)  #install.packages("wordcloud")
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, removePunctuation)
docs
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame, 10), format = "markdown")
par(family=("Heiti TC Light"))
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.1),min.freq=50,max.words=150,
random.order=TRUE, random.color=FALSE,
rot.per=.1, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:3)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction, URL = URL, filename = filename)
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:3)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction, URL = URL, filename = filename)
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:2)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction, URL = URL, filename = filename)
#install.packages("tmcn")
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:2)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction, URL = URL, filename = filename)
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:2)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction, URL = URL, filename = filename)
rm(list=ls(all.names = TRUE))
library(NLP)        # install.packages("NLP")
library(tm)         # install.packages("tm")
library(jiebaRD)    # install.packages("jiebaRD")
library(jiebaR)     # install.packages("jiebaR") 中文文字斷詞
library(RColorBrewer)
library(wordcloud)  #install.packages("wordcloud")
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, removePunctuation)
docs
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame, 10), format = "markdown")
source('D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_4/Course/pttTestFunction.R')
id = c(1:2)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction, URL = URL, filename = filename)
rm(list=ls(all.names = TRUE))
library(NLP)        # install.packages("NLP")
library(tm)         # install.packages("tm")
library(jiebaRD)    # install.packages("jiebaRD")
library(jiebaR)     # install.packages("jiebaR") 中文文字斷詞
library(RColorBrewer)
library(wordcloud)  #install.packages("wordcloud")
filenames <- list.files(getwd(), pattern="*.txt")
files <- lapply(filenames, readLines)
docs <- Corpus(VectorSource(files))
#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs, toSpace, "※")
docs <- tm_map(docs, removePunctuation)
docs
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[order(freqFrame$Freq,decreasing=TRUE), ]
library(knitr)
kable(head(freqFrame, 10), format = "markdown")
par(family=("Heiti TC Light"))
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.1),min.freq=50,max.words=150,
random.order=TRUE, random.color=FALSE,
rot.per=.1, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
if(!file.exists("10.txt")) {
id = c(1:2)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction, URL = URL, filename = filename)
}
if(!file.exists("10.txt")) {
id = c(1:2)
URL = paste0("https://www.ptt.cc/bbs/NTU/index", id, ".html")
filename = paste0(id, ".txt")
pttTestFunction(URL[1], filename[1])
mapply(pttTestFunction, URL = URL, filename = filename)
} else {
print("No")
}
setwd("~/")
install.packages("xlsx")
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
#Set Language as traditional Chinese
Sys.setlocale(category = "LC_ALL", locale = "cht")
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# data clean
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
# words cut
keywords = read.csv("keywords.xlsx")
mixseg = worker()
keys = as.matrix(keywords)
new_user_word(mixseg, keys)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
# tf-idf computation
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
library("xlsx")
encod<-file('show.xlsx',encoding="UTF-8")
write.xlsx(tfidfnn, encod)
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
#Set Language as traditional Chinese
Sys.setlocale(category = "LC_ALL", locale = "cht")
rawData = readtext("*.txt")
setwd("D:/NTU_DataScience (R)/NTU_CSX_DataScience/Week_5/Course")
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# data clean
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
# words cut
keywords = read.csv("keywords.xlsx")
mixseg = worker()
keys = as.matrix(keywords)
new_user_word(mixseg, keys)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
# tf-idf computation
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
library("xlsx")
encod<-file('show.xlsx',encoding="UTF-8")
write.xlsx(tfidfnn, encod)
encod<-file('show.csv',encoding="UTF-8")
write.csv(tfidfnn, encod)
write.table(tfidfnn, encod)
tfidfnn
write.table(tfidfnn)
write.table(tfidfnn, 'show.txt')
write.csv(tfidfnn, 'show.csv')
encod<-file('show.csv',encoding="UTF-8")
write.csv(tfidfnn, encod)
